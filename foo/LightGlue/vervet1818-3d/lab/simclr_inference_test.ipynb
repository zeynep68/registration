{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b407e1dc-60f5-4454-abb1-3925fc9bf9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/p/fastdata/pli/Private/oberstrass1/datasets/vervet1818-3d\n",
      "jrlogin05.jureca\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%cd ..\n",
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f21264f-8fed-47c7-a95f-5c10baedfcb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "import math\n",
    "\n",
    "from vervet1818_3d.resnet_wider import resnet50x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1837f4ac-62f2-4f41-9726-450f10040a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pli_styles.modality.inclination import corr_factor_transmittance_weighted, inclination_from_retardation\n",
    "from pli_styles.modality.fom import hsv_fom\n",
    "\n",
    "from pli.data import Section\n",
    "\n",
    "\n",
    "Coord = namedtuple(\"Coord\", ('x', 'y'))\n",
    "\n",
    "\n",
    "def generate_fom(trans, dir, ret):\n",
    "    corr = corr_factor_transmittance_weighted(\n",
    "        trans,\n",
    "        t_M=0.32,  # 0.23\n",
    "        t_c=0.65,  # 0.65\n",
    "        r_ref_wm=0.96, # 0.96\n",
    "        r_ref_gm=0.16, # 0.16\n",
    "        median_kernel_size=3\n",
    "    )\n",
    "    incl = inclination_from_retardation(\n",
    "        ret,\n",
    "        corr\n",
    "    )\n",
    "    fom = hsv_fom(\n",
    "        np.rad2deg(dir),\n",
    "        incl,\n",
    "        saturation_min=0,\n",
    "        inclination_scale='cosinus'\n",
    "    )\n",
    "    return fom\n",
    "\n",
    "class SectionDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, trans_file, dir_file, ret_file, patch_shape, out_shape, ram=True, norm_trans=None, norm_ret=None):\n",
    "        # Expands the dataset to size input by repeating the provided ROIs\n",
    "        # rois is a list of dicts with entries 'mask', 'ntrans', 'ret' and 'dir'\n",
    "        super().__init__()\n",
    "\n",
    "        # Scale to size that was used in Imagenet training\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "        ])\n",
    "\n",
    "        self.ram = ram\n",
    "        self.trans_section_mod = Section(path=trans_file)\n",
    "        self.dir_section_mod = Section(path=dir_file)\n",
    "        self.ret_section_mod = Section(path=ret_file)\n",
    "        if ram:\n",
    "            print(\"Load sections to RAM...\")\n",
    "            self.trans_section = np.array(self.trans_section_mod.image)\n",
    "            self.dir_section = np.array(self.dir_section_mod.image)\n",
    "            self.ret_section = np.array(self.ret_section_mod.image)\n",
    "            print(\"All sections loaded to RAM\")\n",
    "        else:\n",
    "            print(\"Do not load sections to RAM\")\n",
    "            self.trans_section = self.trans_section_mod.image\n",
    "            self.dir_section = self.dir_section_mod.image\n",
    "            self.ret_section = self.ret_section_mod.image\n",
    "\n",
    "        if norm_trans is None:\n",
    "            if self.trans_section_mod.norm_value is not None:\n",
    "                self.norm_trans = self.trans_section_mod.norm_value\n",
    "            else:\n",
    "                print(\"[WARNING] Did not find a normalization value for Transmittance\")\n",
    "                self.norm_trans = 1.0\n",
    "        else:\n",
    "            self.norm_trans = norm_trans\n",
    "            print(f\"Normalize Transmittance by value of {self.norm_trans}\")\n",
    "        if norm_ret is None:\n",
    "            self.norm_ret = 1.0\n",
    "        else:\n",
    "            self.norm_ret = norm_ret\n",
    "            print(f\"Normalize Retardation by value of {self.norm_ret}\")\n",
    "        self.brain_id = self.trans_section_mod.brain_id\n",
    "        self.section_id = self.trans_section_mod.id\n",
    "        self.section_roi = self.trans_section_mod.roi\n",
    "\n",
    "        assert (patch_shape[0] - out_shape[0]) % 2 == 0  # Border symmetric\n",
    "        assert (patch_shape[1] - out_shape[1]) % 2 == 0  # Border symmetric\n",
    "        self.patch_shape = patch_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.border = ((patch_shape[0] - out_shape[0]) // 2, (patch_shape[1] - out_shape[1]) // 2)\n",
    "        self.shape = self.trans_section.shape\n",
    "\n",
    "        self.coords = [Coord(x=x, y=y) for x in np.arange(0, self.shape[1], out_shape[1]) for y in\n",
    "                       np.arange(0, self.shape[0], out_shape[0])]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.coords[i].x\n",
    "        y = self.coords[i].y\n",
    "\n",
    "        b_y = self.border[0]\n",
    "        b_x = self.border[1]\n",
    "\n",
    "        pad_y_0 = max(b_y - y, 0)\n",
    "        pad_x_0 = max(b_x - x, 0)\n",
    "        pad_y_1 = max(y + (self.patch_shape[0] - b_y) - self.shape[0], 0)\n",
    "        pad_x_1 = max(x + (self.patch_shape[1] - b_x) - self.shape[1], 0)\n",
    "\n",
    "        trans_crop = np.array(\n",
    "            self.trans_section[max(0, y - b_y):min(self.shape[0], y + self.patch_shape[0] - b_y),\n",
    "            max(0, x - b_x):min(self.shape[1], x + self.patch_shape[1] - b_x)],\n",
    "            dtype=np.float32\n",
    "        ) / self.norm_trans\n",
    "        ret_crop = np.array(\n",
    "            self.ret_section[max(0, y - b_y):min(self.shape[0], y + self.patch_shape[0] - b_y),\n",
    "            max(0, x - b_x):min(self.shape[1], x + self.patch_shape[1] - b_x)],\n",
    "            dtype=np.float32\n",
    "        ) / self.norm_ret\n",
    "        dir_crop = np.deg2rad(\n",
    "            self.dir_section[max(0, y - b_y):min(self.shape[0], y + self.patch_shape[0] - b_y),\n",
    "            max(0, x - b_x):min(self.shape[1], x + self.patch_shape[1] - b_x)],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        fom_crop = generate_fom(trans_crop, dir_crop, ret_crop)\n",
    "\n",
    "        fom_crop = np.pad(fom_crop, ((pad_y_0, pad_y_1), (pad_x_0, pad_x_1), (0, 0)), mode='constant', constant_values=0.0)\n",
    "        \n",
    "        fom_crop = self.transforms(fom_crop)\n",
    "       \n",
    "        return {'x': x, 'y': y, 'crop': fom_crop}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f9fc577f-47c4-44bb-acdb-1ca85681bd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Tuple\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_files(\n",
    "        trans: str,\n",
    "        dir: str,\n",
    "        ret: str,\n",
    "        out: str,\n",
    "        rank: int = 0,\n",
    "        size: int = 1\n",
    "):\n",
    "    print(trans)\n",
    "    trans_files = sorted(glob(trans))\n",
    "    dir_files = sorted(glob(dir))\n",
    "    ret_files = sorted(glob(ret))\n",
    "\n",
    "    if os.path.isdir(out):\n",
    "        ft_files = []\n",
    "        for d_f in dir_files:\n",
    "            d_fname = os.path.splitext(os.path.basename(d_f))[0]\n",
    "            d_base = os.path.splitext(d_fname)[0]\n",
    "            ft_file = re.sub(\"direction\", \"Features\", d_base, flags=re.IGNORECASE)\n",
    "            if \"Features\" not in ft_file:\n",
    "                ft_file += \"_Features.h5\"\n",
    "            else:\n",
    "                ft_file += \".h5\"\n",
    "            ft_files.append(os.path.join(out, ft_file))\n",
    "    else:\n",
    "        ft_files = [out]\n",
    "\n",
    "    for i, (trans_file, dir_file, ret_file, ft_file) \\\n",
    "            in enumerate(zip(trans_files, dir_files, ret_files, ft_files)):\n",
    "        if i % size == rank:\n",
    "            if not os.path.isfile(ft_file):\n",
    "                yield trans_file, dir_file, ret_file, ft_file\n",
    "            else:\n",
    "                print(f\"{ft_file} already exists. Skip.\")\n",
    "\n",
    "\n",
    "def create_features(\n",
    "        encoder: torch.nn.Module,\n",
    "        section_loader: DataLoader,\n",
    "        h_size: int,\n",
    "        out_size: Tuple[int, ...],\n",
    "        stride: Tuple[int, ...],\n",
    "        rank: int\n",
    "):\n",
    "    print(\"Initialize output featuremaps...\")\n",
    "    h_features = np.zeros((*out_size, h_size), dtype=np.float32)\n",
    "    \n",
    "    def get_outputs(batch, network):\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            h = network(\n",
    "                batch['crop'].to(network.device),\n",
    "            )\n",
    "        return {'x': batch['x'], 'y': batch['y'], 'h': h}\n",
    "\n",
    "    def transfer(batch, network):\n",
    "        b = get_outputs(batch, network)\n",
    "        for x, y, h in zip(b['x'], b['y'], b['h']):\n",
    "            try:\n",
    "                h_features[y // stride[0], x // stride[1]] = h.cpu().numpy()\n",
    "            except:\n",
    "                raise Exception(f\"ERROR creating mask at x={x}, y={y}, shape={h_features.shape}\")\n",
    "\n",
    "    print(\"Start feature generation...\")\n",
    "    for batch in tqdm(section_loader, desc=f\"Rank {rank}\"):\n",
    "        transfer(batch, encoder)\n",
    "\n",
    "    return h_features\n",
    "\n",
    "\n",
    "def save_features(\n",
    "        h_features: np.ndarray,\n",
    "        z_features: np.ndarray,\n",
    "        ft_file: str,\n",
    "        spacing: Tuple[float, ...] = (1.0, 1.0),\n",
    "        origin: Tuple[float, ...] = (0.0, 0.0),\n",
    "        dtype: str = None,\n",
    "):\n",
    "    print(\"Save features...\")\n",
    "    with h5.File(ft_file, \"w\") as f:\n",
    "        feature_group = f.create_group(\"Features\")\n",
    "        dset_h = feature_group.create_dataset(f\"{h_features.shape[-1]}\", data=h_features.transpose(2, 0, 1), dtype=dtype)\n",
    "        dset_h.attrs['spacing'] = spacing\n",
    "        dset_h.attrs['origin'] = origin\n",
    "        dset_z = feature_group.create_dataset(f\"{z_features.shape[-1]}\", data=z_features.transpose(2, 0, 1), dtype=dtype)\n",
    "        dset_z.attrs['spacing'] = spacing\n",
    "        dset_z.attrs['origin'] = origin\n",
    "    print(f\"Featuremaps created at {ft_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932c19be-60ff-4a30-a05d-0df149bb9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = 'models/simclr_converter/resnet50-1x.pth'\n",
    "device = 'cpu'\n",
    "\n",
    "###\n",
    "\n",
    "encoder = resnet50x1()\n",
    "sd = torch.load(sd, map_location='cpu')\n",
    "encoder.load_state_dict(sd['state_dict'])\n",
    "\n",
    "encoder.device = device\n",
    "encoder.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95ece24-60f6-4fe3-94c6-629b93f8c9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "overlap = 0.0\n",
    "\n",
    "###\n",
    "\n",
    "patch_shape = (patch_size, patch_size)\n",
    "stride = (int((1 - overlap) * patch_size), int((1 - overlap) * patch_size))\n",
    "\n",
    "h_size = encoder.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44883f05-ad5a-4295-8573-6a4e91529b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_*NTransmittance.nii.gz\n",
      "Initialize DataLoader for in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_60mu_70ms_s0841_x00-21_y00-14_NTransmittance.nii.gz, in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_60mu_70ms_s0841_x00-21_y00-14_Direction.nii.gz, in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_60mu_70ms_s0841_x00-21_y00-14_Retardation.nii.gz\n",
      "Do not load sections to RAM\n",
      "[WARNING] Did not find a normalization value for Transmittance\n",
      "Initialize output featuremaps...\n",
      "Start feature generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rank 0:   0%|          | 0/54675 [00:00<?, ?it/s]/tmp/ipykernel_3236/1191376103.py:67: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  h_features[y // stride[0], x // stride[1]] = h.cpu().numpy()\n",
      "Rank 0:   0%|          | 2/54675 [00:27<211:36:10, 13.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3236/2761594555.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mh_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mspacing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_section_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3236/1191376103.py\u001b[0m in \u001b[0;36mcreate_features\u001b[0;34m(encoder, section_loader, h_size, out_size, stride, rank)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start feature generation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Rank {rank}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3236/1191376103.py\u001b[0m in \u001b[0;36mtransfer\u001b[0;34m(batch, network)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3236/1191376103.py\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(batch, network)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             h = network(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2022/software/PyTorch/1.11-gcccoremkl-11.2.0-2021.4.0-CUDA-11.5/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/fastdata/pli/Private/oberstrass1/datasets/vervet1818-3d/src/vervet1818_3d/resnet_wider.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/fastdata/pli/Private/oberstrass1/datasets/vervet1818-3d/src/vervet1818_3d/resnet_wider.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2022/software/PyTorch/1.11-gcccoremkl-11.2.0-2021.4.0-CUDA-11.5/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2022/software/PyTorch/1.11-gcccoremkl-11.2.0-2021.4.0-CUDA-11.5/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p/software/jurecadc/stages/2022/software/PyTorch/1.11-gcccoremkl-11.2.0-2021.4.0-CUDA-11.5/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trans = \"in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_*NTransmittance.nii.gz\"\n",
    "dir = \"in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_*Direction.nii.gz\"\n",
    "ret = \"in/volume-reconstruction/Transmittance_Retardation/nifti/Vervet1818aa_*Retardation.nii.gz\"\n",
    "  \n",
    "out = \"data/aa/features/simclr-imagenet\"\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "rank = 0\n",
    "size = 1\n",
    "\n",
    "ram = False\n",
    "dtype = \"float16\"\n",
    "\n",
    "###\n",
    "\n",
    "for trans_file, dir_file, ret_file, ft_file in get_files(trans, dir, ret, out, rank, size):\n",
    "    print(f\"Initialize DataLoader for {trans_file}, {dir_file}, {ret_file}\")\n",
    "\n",
    "    section_dataset = SectionDataset(\n",
    "        trans_file=trans_file,\n",
    "        dir_file=dir_file,\n",
    "        ret_file=ret_file,\n",
    "        patch_shape=patch_shape,\n",
    "        out_shape=stride,\n",
    "        ram=ram,\n",
    "    )\n",
    "    section_loader = DataLoader(\n",
    "        section_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    out_size = tuple(math.ceil(s / stride[i]) for i, s in enumerate(section_dataset.shape[:2]))\n",
    "\n",
    "    h_features = create_features(encoder, section_loader, h_size, out_size, stride, rank)\n",
    "\n",
    "    spacing = tuple(stride[i] * s for i, s in enumerate(section_dataset.trans_section_mod.spacing))\n",
    "    origin = section_dataset.trans_section_mod.origin\n",
    "\n",
    "    save_features(h_features, ft_file, spacing, origin, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7705a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv (pli-env)",
   "language": "python",
   "name": "pli-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
